# -*- coding: utf-8 -*-
"""random.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lrqfe3tCC1dO43ZxoD0-SLqES5ObxBGt
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import datetime

import random
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import argmax

from sklearn.model_selection import train_test_split

def create_input():
  __dict = {};
  __dict['age'] = random.randint(10, 40)
  __dict['gender'] = np.random.choice(['Male', 'Female'], 1, [0.55, 0.45])[0]
  __dict['emotion'] = np.random.choice(
      ['Happy', 'Sad', 'Fear', 'Disgust', 'Anger', 'Surprice'], 1, 
      [0.23, 0.18, 0.13, 0.1, 0.19, 0.17])[0]
  __dict['color'] = random.choice(['Red', 'Blue', 'Green', 'White', 'Gray'])
  __dict['animal'] = random.choice(['Tiger', 'Fish', 'Dog', 'Eagal', 'Horse'])
  __dict['q3'] = random.choice(['N1', 'N2', 'N3', 'N4', 'N5'])
  __dict['q4'] = random.choice(['M1', 'M2', 'M3', 'M4', 'M5'])
  return __dict

data = [];
for i in range(10000):
  _dict = create_input()
  _dict['target'] = random.randint(0, 39)
  data.append(_dict)

df = pd.DataFrame(data)

df.head()

# df = pd.read_csv('dataset.csv')

train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  labels_one_hot = to_categorical(labels, num_classes=40)
  
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels_one_hot))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 20
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_columns = []
fc_age = feature_column.numeric_column('age')
fc_age_buckets = feature_column.bucketized_column(fc_age, boundaries=[20, 30])
feature_columns.append(fc_age_buckets)

fc_gender = feature_column.categorical_column_with_vocabulary_list( 'gender', ['Male', 'Female'])
fc_gender_one_hot = feature_column.indicator_column(fc_gender)
feature_columns.append(fc_gender_one_hot)

fc_emotion = feature_column.categorical_column_with_vocabulary_list( 'emotion', ['Happy', 'Sad', 'Fear', 'Disgust', 'Anger', 'Surprice'])
fc_emotion_one_hot = feature_column.indicator_column(fc_emotion)
feature_columns.append(fc_emotion_one_hot)

fc_color = feature_column.categorical_column_with_vocabulary_list( 'color', ['Red', 'Blue', 'Green', 'White', 'Gray'])
fc_color_one_hot = feature_column.indicator_column(fc_color)
feature_columns.append(fc_color_one_hot)

fc_animal = feature_column.categorical_column_with_vocabulary_list( 'animal', ['Tiger', 'Fish', 'Dog', 'Eagal', 'Horse'])
fc_animal_one_hot = feature_column.indicator_column(fc_animal)
feature_columns.append(fc_animal_one_hot)

fc_q3 = feature_column.categorical_column_with_vocabulary_list( 'q3', ['N1', 'N2', 'N3', 'N4', 'N5'])
fc_q3_one_hot = feature_column.indicator_column(fc_q3)
feature_columns.append(fc_q3_one_hot)

fc_q4 = feature_column.categorical_column_with_vocabulary_list( 'q4', ['M1', 'M2', 'M3', 'M4', 'M5'])
fc_q4_one_hot = feature_column.indicator_column(fc_q4)
feature_columns.append(fc_q4_one_hot)
feature_columns

def create_model(_feature_columns):
  _feature_layer = layers.DenseFeatures(_feature_columns)

  _model = tf.keras.models.Sequential([
    _feature_layer,
    layers.Dense(100, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(40, activation='softmax')
  ])
  
  _model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                metrics=[tf.metrics.CategoricalCrossentropy()])
  
  return _model

model = create_model(feature_columns)

# !rm -rf ./logs/
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[tensorboard_callback])

model.evaluate(test_ds)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

# !mkdir -p saved_model
model.save('saved_model/my_model') 

# save HDF5 file
# model.save('my_model.h5')

# import model test
new_model = tf.keras.models.load_model('saved_model/my_model')

# load HDF5 file
# new_model = tf.keras.models.load_model('my_model.h5')

new_model.build(input_shape={})

new_model.summary()

# Predict

dict1 = {
  'age': 25,
  'gender': 'Male',
  'emotion': 'Happy',
  'color': 'Blue',
  'animal': 'Horse',
  'q3': 'N1',
  'q4': 'M1'
}

dict2 = {
  'age': 26,
  'gender': 'Female',
  'emotion': 'Surprice',
  'color': 'Red',
  'animal': 'Fish',
  'q3': 'N1',
  'q4': 'M1'
}

pred_dicts = [dict1, dict2]
# for pi in range(10):
#   pred_dicts.append(create_input())

pred_ds1 = tf.data.Dataset.from_tensor_slices(dict(pd.DataFrame(pred_dicts)))
pred_ds1 = pred_ds1.batch(32)

# # list(pres_ds1.as_numpy_iterator())
predicts = model.predict(pred_ds1)

for index, predict in enumerate(predicts):
  print(pred_dicts[index], np.argmax(predict))

# print('')
# print('==============')
# print('')

# predicts = new_model.predict(pred_ds1)

# for index, predict in enumerate(predicts):
#   print(pred_dicts[index], np.argmax(predict))